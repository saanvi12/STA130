{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee535d1",
   "metadata": {},
   "source": [
    "## 1.\n",
    "1. Simple vs. Multiple Linear Regression\n",
    "\n",
    "\t•\tSimple Linear Regression: Models the outcome ￼ using a single predictor ￼:\n",
    "￼\n",
    "Useful for analyzing one predictor’s effect on ￼.\n",
    "\t•\tMultiple Linear Regression: Uses several predictors to explain ￼:\n",
    "￼\n",
    "Advantage: It allows you to consider multiple factors, giving a more comprehensive model.\n",
    "\n",
    "2. Continuous vs. Indicator Variables in Simple Linear Regression\n",
    "\n",
    "\t•\tContinuous Variable: A predictor that can take on any numeric value. With a continuous ￼:\n",
    "￼\n",
    "￼ changes continuously with ￼.\n",
    "\t•\tIndicator Variable: A binary predictor (0 or 1) representing group membership:\n",
    "￼\n",
    "This creates a categorical shift in ￼ based on group (rather than a continuous trend).\n",
    "\n",
    "3. Adding an Indicator Variable in Multiple Linear Regression\n",
    "\n",
    "Combining a continuous and indicator variable in the same model:\n",
    "￼\n",
    "produces parallel lines for each group defined by the indicator. The slope for ￼ is the same, but each group has its own intercept.\n",
    "\n",
    "4. Interaction Between Continuous and Indicator Variables\n",
    "\n",
    "Adding an interaction term changes the relationship between the continuous variable and ￼ based on the group:\n",
    "￼\n",
    "Now, each group defined by ￼ has its own slope and intercept, allowing the lines to diverge based on group.\n",
    "\n",
    "5. Multiple Linear Regression with Indicator Variables for Categorical Variables\n",
    "\n",
    "For a categorical variable with ￼ categories, we use ￼ indicators. For example:\n",
    "￼\n",
    "\t•\tThe intercept ￼ is the baseline (e.g., first category).\n",
    "\t•\tEach indicator shifts ￼ relative to this baseline.\n",
    "\n",
    "This approach helps compare each category against a reference, giving each category a unique intercept while avoiding redundancy (only using ￼ indicators)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6c8c8",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "Outcome and Predictor Variables\n",
    "\n",
    "\t•\tOutcome Variable: Sales of sports equipment (￼).\n",
    "\t•\tPredictor Variables:\n",
    "\t•\tTV Ad Spend (￼): Represents the amount spent on TV advertising.\n",
    "\t•\tOnline Ad Spend (￼): Represents the amount spent on online advertising.\n",
    "\n",
    "Interaction Consideration\n",
    "\n",
    "\t•\tPotential Interaction: The effectiveness of TV ad spend on sales may depend on the level of online ad spend, and vice versa. This interaction suggests that if both ad spends are high, the combined effect on sales could be different (e.g., synergistic) than if each ad spend were considered in isolation.\n",
    "\n",
    "Linear Forms with and without Interaction\n",
    "\n",
    "\t1.\tWithout Interaction (Additive Model):\n",
    "￼\n",
    "Here, the effects of TV and online ad spending are independent of each other.\n",
    "\t2.\tWith Interaction (Interactive Model):\n",
    "￼\n",
    "The interaction term ￼ allows the effect of each type of ad spend to vary based on the level of the other. This could mean, for example, that high spending on both platforms has an amplified effect on sales.\n",
    "\n",
    "Using These Models for Predictions\n",
    "\n",
    "\t•\tWithout Interaction: Predict sales by adding the separate contributions of TV and online ad spends. Each has a fixed effect on ￼, regardless of the level of the other.\n",
    "\t•\tWith Interaction: Predict sales by accounting for both individual and combined effects. Here, if both ad spends are high, the interaction could amplify the impact on ￼, giving different sales outcomes than the additive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98ed16",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "Interaction Model\n",
    "\n",
    "\t•\tLinear Form:  \\text{logit}(P) = \\beta_0 + \\beta_1 \\text{age} + \\beta_2 \\text{employment\\_status} + \\beta_3 (\\text{age} \\times \\text{employment\\_status}) \n",
    "\t•\tInterpretation:\n",
    "\t•\t \\beta_3 : Captures how the effect of age on log odds depends on employment status.\n",
    "\t•\tIf significant, the slopes of the lines (age effect) differ for employed and unemployed groups.\n",
    "\n",
    "Visualization\n",
    "\n",
    "\t•\tAdditive Model Plot: Predicts parallel relationships between age and probability of high connection for employed and unemployed groups.\n",
    "\t•\tInteraction Model Plot: Allows the relationship between age and high connection probability to vary by employment status.\n",
    "\n",
    "Statistical Evidence\n",
    "\n",
    "\t•\tUse the .summary() output to evaluate p-values for each coefficient. If  \\beta_3  (interaction term) is significant, the interaction model better captures the relationship between predictors and the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac7ddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52/1129365582.py:7: DtypeWarning: Columns (408,1001,1002,1006,1007,1008,1080,1113,1115,1116,1117,1118,1119,1120,1121,1124,1125,1126,1127,1128,1213,1214,1215,1216,1217,1218,1342,1343,1344,1345,1346,1347,1348,1349,1390,1391,1393,1463,1549,1552,1555,1558,1561) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"CSCS_data_anon.csv\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'social_connection_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'social_connection_level'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSCS_data_anon.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a binary outcome variable\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming 'social_connection_level' is a categorical column with values like 'high', 'low'\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh_connection\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msocial_connection_level\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Ensure predictors are prepared (e.g., binary or continuous)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Example: 'employment_status' (binary) and 'age' (continuous)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployment_status\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Drop rows with missing values\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'social_connection_level'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the dataset (replace the path with the actual file location or URL if available)\n",
    "data = pd.read_csv(\"CSCS_data_anon.csv\")\n",
    "\n",
    "# Create a binary outcome variable\n",
    "# Assuming 'social_connection_level' is a categorical column with values like 'high', 'low'\n",
    "data['high_connection'] = (data['social_connection_level'] == 'high').astype(int)\n",
    "\n",
    "# Ensure predictors are prepared (e.g., binary or continuous)\n",
    "# Example: 'employment_status' (binary) and 'age' (continuous)\n",
    "data = data.dropna(subset=['age', 'employment_status'])  # Drop rows with missing values\n",
    "data['employment_status'] = (data['employment_status'] == 'employed').astype(int)\n",
    "# Additive logistic regression model\n",
    "additive_formula = 'high_connection ~ age + employment_status'\n",
    "additive_model = smf.logit(additive_formula, data=data).fit()\n",
    "print(additive_model.summary())\n",
    "# Interaction logistic regression model\n",
    "interaction_formula = 'high_connection ~ age * employment_status'\n",
    "interaction_model = smf.logit(interaction_formula, data=data).fit()\n",
    "print(interaction_model.summary())\n",
    "# Simulate continuous predictor (age) range for predictions\n",
    "ages = np.linspace(data['age'].min(), data['age'].max(), 100)\n",
    "\n",
    "# Predict probabilities for the additive model\n",
    "employed_additive_probs = additive_model.predict(pd.DataFrame({'age': ages, 'employment_status': 1}))\n",
    "unemployed_additive_probs = additive_model.predict(pd.DataFrame({'age': ages, 'employment_status': 0}))\n",
    "\n",
    "# Predict probabilities for the interaction model\n",
    "employed_interaction_probs = interaction_model.predict(pd.DataFrame({'age': ages, 'employment_status': 1}))\n",
    "unemployed_interaction_probs = interaction_model.predict(pd.DataFrame({'age': ages, 'employment_status': 0}))\n",
    "fig_interaction = go.Figure()\n",
    "\n",
    "fig_interaction.add_trace(go.Scatter(\n",
    "    x=ages, y=employed_interaction_probs, mode='lines', name='Employed (Interaction)'\n",
    "))\n",
    "fig_interaction.add_trace(go.Scatter(\n",
    "    x=ages, y=unemployed_interaction_probs, mode='lines', name='Unemployed (Interaction)', line=dict(dash='dash')\n",
    "))\n",
    "\n",
    "fig_interaction.update_layout(\n",
    "    title=\"Interaction Model: Predicted Probabilities\",\n",
    "    xaxis_title=\"Age\",\n",
    "    yaxis_title=\"Probability of High Connection\",\n",
    "    legend_title=\"Employment Status\"\n",
    ")\n",
    "fig_interaction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b839b",
   "metadata": {},
   "source": [
    "## 4. \n",
    "\n",
    "\t1.\tLow ￼: The model explains only 17.6% of the variation in the outcome, suggesting limited overall explanatory power.\n",
    "\t2.\tSignificant Coefficients: Large coefficients with small p-values indicate strong evidence that specific predictors are associated with the outcome.\n",
    "\n",
    "\t•\t￼ reflects the overall model’s ability to explain variability in ￼, which can be low if important predictors are missing or the outcome is highly random.\n",
    "\t•\tP-values test the statistical significance of individual predictors, showing whether a predictor has a measurable effect on ￼, even if the overall model is weak.\n",
    "    \n",
    "    These metrics address different questions. Low ￼ means the model is incomplete, while significant p-values show that the included predictors are relevant. Both can coexist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b014f9",
   "metadata": {},
   "source": [
    "## 5.\n",
    "\n",
    "The code demonstrates how to evaluate a model’s generalizability by comparing in-sample and out-of-sample ￼, highlighting the trade-off between simplicity and complexity, and the risks of overfitting.\n",
    "\n",
    "Cell-by-Cell Explanation\n",
    "\n",
    "\t1.\tData Splitting:\n",
    "\t•\tSplits the dataset into 50% training and 50% testing sets to assess the model’s performance on unseen data.\n",
    "\t•\tEnsures reproducibility using a random seed.\n",
    "\t2.\tSimple Model Fit:\n",
    "\t•\tFits a model predicting ￼ using ￼ and ￼ as predictors.\n",
    "\t•\t￼ from this model reflects how well it explains variability in the training set.\n",
    "\t3.\tSimple Model Evaluation:\n",
    "\t•\tCompares the model’s performance in the training data (in-sample ￼) with the test data (out-of-sample ￼).\n",
    "\t•\tA large gap between these metrics suggests overfitting.\n",
    "\t4.\tComplex Model Fit:\n",
    "\t•\tAdds many predictors and interaction terms, creating a more flexible but also more complex model.\n",
    "\t•\tLikely achieves a higher in-sample ￼ due to capturing more patterns, including noise.\n",
    "\t5.\tComplex Model Evaluation:\n",
    "\t•\tAssesses generalizability by comparing in-sample ￼ to out-of-sample ￼.\n",
    "\t•\tIf out-of-sample ￼ drops significantly, the model is overfit and fails to generalize.\n",
    "\n",
    "Key Insights\n",
    "\n",
    "\t•\tIn-sample ￼ shows how well the model fits the training data.\n",
    "\t•\tOut-of-sample ￼ measures how well the model predicts new, unseen data.\n",
    "\t•\tSimple models tend to generalize better but might miss important patterns.\n",
    "\t•\tComplex models fit the training data well but risk overfitting and performing poorly on test data.\n",
    "\n",
    "This code highlights why evaluating both in-sample and out-of-sample performance is crucial for building models that generalize effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e5571",
   "metadata": {},
   "source": [
    "## 6. \n",
    "The model4_linear_form generates many new predictors (interaction terms) in the design matrix, creating a highly complex representation of the data. This complexity introduces multicollinearity—strong correlations among predictors—which destabilizes the model coefficients. As a result, the model fits the training data too closely (overfitting), leading to poor generalization to unseen data, evidenced by a significant drop in out-of-sample  R^2 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07434877",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\t•\tModel 5 balances simplicity (Model 3) with complexity (Model 4).\n",
    "\t•\tModel 6 expands Model 5 by testing additional terms based on data-driven or theoretical insights.\n",
    "\t•\tModel 7 streamlines Model 6 by focusing on the most significant terms, aiming for a generalizable and interpretable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01339e",
   "metadata": {},
   "source": [
    "## 8. \n",
    "This approach highlights the variability in model performance due to randomness in training/test splits. By comparing in-sample and out-of-sample R^2 across many iterations, we can better understand the model’s reliability and generalization, avoiding conclusions based on a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4039cb7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pokeaman' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Loop to repeat model fitting and evaluation multiple times\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# 100 iterations\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Randomly split data into training and testing sets (no fixed seed here)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mpokeaman\u001b[49m, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Fit a simple model (adjust formula to your context)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     model \u001b[38;5;241m=\u001b[39m smf\u001b[38;5;241m.\u001b[39mols(formula\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHP ~ Attack + Defense\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mtrain)\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pokeaman' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Placeholder for the dataset (replace 'pokeaman' with your actual dataset variable)\n",
    "# pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "# Initialize lists to store in-sample and out-of-sample R^2 values\n",
    "in_sample_r2 = []\n",
    "out_of_sample_r2 = []\n",
    "\n",
    "# Loop to repeat model fitting and evaluation multiple times\n",
    "for i in range(100):  # 100 iterations\n",
    "    # Randomly split data into training and testing sets (no fixed seed here)\n",
    "    train, test = train_test_split(pokeaman, train_size=0.5)\n",
    "    \n",
    "    # Fit a simple model (adjust formula to your context)\n",
    "    model = smf.ols(formula='HP ~ Attack + Defense', data=train).fit()\n",
    "    \n",
    "    # Calculate in-sample R^2\n",
    "    in_sample_r2.append(model.rsquared)\n",
    "    \n",
    "    # Calculate out-of-sample R^2\n",
    "    y_test = test['HP']\n",
    "    yhat_test = model.predict(test)\n",
    "    out_of_sample_r2.append(np.corrcoef(y_test, yhat_test)[0, 1]**2)\n",
    "\n",
    "# Plot in-sample vs. out-of-sample R^2\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(in_sample_r2, out_of_sample_r2, alpha=0.7)\n",
    "plt.axline((0, 0), (1, 1), linestyle='--', color='gray', label='Ideal Generalization')\n",
    "plt.xlabel(\"In-Sample R^2\")\n",
    "plt.ylabel(\"Out-of-Sample R^2\")\n",
    "plt.title(\"In-Sample vs. Out-of-Sample R^2 Across Iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af359ca2",
   "metadata": {},
   "source": [
    "## 9.\n",
    "This code tests how well models trained on Pokémon data from specific generations can predict stats for other generations. It compares the model’s performance in three scenarios:\n",
    "\t1.\tOriginal Model: The model is trained and tested on the standard train-test split (all generations mixed). This gives a baseline for in-sample (training data) and out-of-sample (test data) performance.\n",
    "\t2.\tModel Trained Only on Generation 1: The model is trained on just Generation 1 Pokémon and then used to predict stats for Pokémon from other generations. It fits the Generation 1 data well (high in-sample R^2) but likely struggles to predict other generations accurately (low out-of-sample R^2) because their stats may differ significantly.\n",
    "\t3.\tModel Trained on Generations 1-5: The model is trained on Pokémon from Generations 1-5 and used to predict stats for Generation 6. This setup tests whether training on a broader set of data helps the model generalize better to unseen data (Generation 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da923cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
