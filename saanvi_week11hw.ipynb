{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a772788",
   "metadata": {},
   "source": [
    "## 1.\n",
    "(a) \n",
    "A Classification Decision Tree solves problems involving categorical outcomes (e.g., spam vs. non-spam, disease vs. no disease). It works by sequentially splitting data based on features to group similar data points. Applications include healthcare diagnosis, fraud detection, customer segmentation, and spam filtering.\n",
    "\n",
    "(b) \n",
    "A Classification Decision Tree predicts by sequentially evaluating feature-based questions at each node and following branches until it reaches a leaf node, where it assigns the most frequent class. Multiple Linear Regression predicts a continuous value using a linear equation that combines feature contributions as weighted sums. Decision Trees handle interactions implicitly through splits, while regression requires explicit interaction terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145dbf6",
   "metadata": {},
   "source": [
    "## 2.\n",
    "    1.\tAccuracy:\n",
    "Scenario: General classification problems where false positives and false negatives have similar consequences (e.g., spam detection).\n",
    "Rationale: Accuracy provides an overall performance metric but can be misleading when classes are imbalanced, as it treats all predictions equally.\n",
    "\n",
    "\t2.\tSensitivity (Recall):\n",
    "Scenario: Medical diagnostics, such as cancer detection, where identifying all true positives is critical.\n",
    "Rationale: Sensitivity ensures that most actual positives are caught, even if some negatives are misclassified, minimizing the risk of missing a diagnosis.\n",
    "\n",
    "\t3.\tSpecificity:\n",
    "Scenario: Fraud detection systems where the cost of false positives is high (e.g., flagging legitimate transactions as fraud).\n",
    "Rationale: High specificity ensures that most actual negatives are correctly identified, reducing unnecessary disruptions or false alarms.\n",
    "\n",
    "\t4.\tPrecision:\n",
    "Scenario: Email spam filters, where delivering legitimate emails (not false positives) is crucial.\n",
    "Rationale: Precision focuses on minimizing false positives, ensuring that flagged results are mostly accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f43ac68",
   "metadata": {},
   "source": [
    "## 3. \n",
    "EDA\n",
    "\n",
    "Numerical Data\n",
    "\n",
    "\t•\tA detailed summary of numerical columns like List Price, Amazon Price, NumPages, Pub year, and Thick is available.\n",
    "\t•\tThis includes statistics like mean, median, minimum, and maximum values.\n",
    "\n",
    "Categorical Data\n",
    "\n",
    "\t•\tThe dataset contains one categorical column (Hard_or_Paper), which represents whether a book is hardcover or paperback.\n",
    "\n",
    "Key Observations\n",
    "\n",
    "\t•\tUnique Values:\n",
    "\t•\tThe dataset has 309 unique book titles, 251 unique authors, and 158 unique publishers.\n",
    "\t•\tOnly 2 unique values for Hard_or_Paper (likely “H” and “P”).\n",
    "\t•\tData Completeness:\n",
    "\t•\tAfter preprocessing, the dataset has 319 rows and no missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b96ca6",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\t•\tThe decision tree splits the List Price into ranges to determine whether a book is classified as hardcover or paperback.\n",
    "\t•\tEach node represents a split based on the List Price value.\n",
    "\t•\tLeaf nodes provide the predicted class (Paperback or Hardcover) and the probability distribution of the classes in that split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62355957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create an 80/20 train-test split with a random seed for reproducibility\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ab_reduced_noNaN_train, ab_reduced_noNaN_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mab_reduced_noNaN\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Report the size of the training and test sets\u001b[39;00m\n\u001b[1;32m     12\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ab_reduced_noNaN_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an 80/20 train-test split with a random seed for reproducibility\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(\n",
    "    ab_reduced_noNaN, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Report the size of the training and test sets\n",
    "train_size = len(ab_reduced_noNaN_train)\n",
    "test_size = len(ab_reduced_noNaN_test)\n",
    "\n",
    "# Prepare data for model training\n",
    "y = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']  # Binary target for \"Hardcover\"\n",
    "X = ab_reduced_noNaN_train[['List Price']]  # Feature\n",
    "\n",
    "# Train a DecisionTreeClassifier with max_depth=2\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(10, 8))\n",
    "tree.plot_tree(clf, feature_names=['List Price'], class_names=['Paperback', 'Hardcover'], filled=True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "y_test = pd.get_dummies(ab_reduced_noNaN_test[\"Hard_or_Paper\"])['H']\n",
    "X_test = ab_reduced_noNaN_test[['List Price']]\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy = (predictions == y_test).mean()\n",
    "\n",
    "# Display key information\n",
    "train_size, test_size, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c7a7c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Visualize the decision tree\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m tree\u001b[38;5;241m.\u001b[39mplot_tree(\u001b[43mclf2\u001b[49m, \n\u001b[1;32m      8\u001b[0m                feature_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumPages\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThick\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList Price\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      9\u001b[0m                class_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaperback\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHardcover\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     10\u001b[0m                filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf2' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#5.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(16, 10))\n",
    "tree.plot_tree(clf2, \n",
    "               feature_names=['NumPages', 'Thick', 'List Price'], \n",
    "               class_names=['Paperback', 'Hardcover'], \n",
    "               filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57835a6",
   "metadata": {},
   "source": [
    "## 6.\n",
    "Model clf (List Price only):\n",
    "\n",
    "\t•\tAccuracy: 0.844\n",
    "\t•\tSensitivity (Recall for Hardcover): 0.700\n",
    "\t•\tSpecificity (Recall for Paperback): 0.909\n",
    "\n",
    "Model clf2 (NumPages, Thick, List Price):\n",
    "\n",
    "\t•\tAccuracy: 0.859\n",
    "\t•\tSensitivity (Recall for Hardcover): 0.750\n",
    "\t•\tSpecificity (Recall for Paperback): 0.909"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc84e4",
   "metadata": {},
   "source": [
    "## 7. \n",
    "The differences occur because the first confusion matrix evaluates clf on data it was trained on (List Price), while the second applies clf to data with extra features it wasn’t trained to handle, leading to unreliable predictions. The confusion matrices for clf and clf2 are better because each model is tested on data consistent with its training, producing meaningful and accurate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606bd9d8",
   "metadata": {},
   "source": [
    "## 8.\n",
    "The most important predictor variable for making predictions according to clf2 is List Price, as it contributes the most to the overall explanatory power of the classification decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87b2f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Visualize feature importances using a bar plot\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[43mclf2\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_names_in_, clf2\u001b[38;5;241m.\u001b[39mfeature_importances_, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskyblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature Importances for clf2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf2' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize feature importances using a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(clf2.feature_names_in_, clf2.feature_importances_, color='skyblue')\n",
    "plt.title('Feature Importances for clf2')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41497abe",
   "metadata": {},
   "source": [
    "## 9.\n",
    "In linear regression, coefficients directly represent the change in the predicted outcome for a one-unit change in a predictor variable, making interpretation straightforward and additive. In contrast, feature importances in decision trees indicate the relative contribution of each feature to improving prediction accuracy (e.g., reducing Gini impurity or entropy) across all splits in the tree, but they do not quantify a direct effect or direction of influence. This makes feature importance less intuitive compared to linear regression coefficients, as the relationships in decision trees are more complex and involve hierarchical and interactive effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ec4e8",
   "metadata": {},
   "source": [
    "## 10.\n",
    "Yes\n",
    "Summaries:\n",
    "Here’s a summary of our conversation:\n",
    "\t1.\tClassification Decision Trees:\n",
    "\t•\tExplored their purpose (categorical predictions), applications (e.g., spam filtering, fraud detection), and comparison with regression models.\n",
    "\t2.\tAmazon Books Dataset Preprocessing:\n",
    "\t•\tCleaned the dataset by removing specific columns, handling missing data, and setting appropriate data types.\n",
    "\t3.\tExploratory Data Analysis (EDA):\n",
    "\t•\tConducted basic EDA, summarizing numerical and categorical data, and identifying unique values.\n",
    "\t4.\tTrain-Test Split:\n",
    "\t•\tCreated an 80/20 train-test split and trained a decision tree (clf) using List Price as the predictor.\n",
    "\t5.\tTree Visualization:\n",
    "\t•\tVisualized clf to explain its prediction logic based on splits in List Price.\n",
    "\t6.\tEnhanced Model (clf2):\n",
    "\t•\tTrained another decision tree using NumPages, Thick, and List Price with a max depth of 4, and visualized it.\n",
    "\t7.\tConfusion Matrices:\n",
    "\t•\tGenerated and analyzed confusion matrices for both clf and clf2, reporting sensitivity, specificity, and accuracy.\n",
    "\t8.\tFeature Importances:\n",
    "\t•\tVisualized feature importances for clf2, identifying List Price as the most important predictor.\n",
    "\t9.\tInterpreting Models:\n",
    "\t•\tCompared how linear regression coefficients directly show effects, while decision tree feature importances reflect overall contribution through hierarchical splits.\n",
    "\n",
    "Let me know if you need further clarification or additional analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b0aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
